{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4dtFGbI60W7D",
        "outputId": "d4e25f5d-73ec-4c32-e3fe-8b62ad34436c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "zsy96C0yU0yC",
        "outputId": "81ea3c24-d1e6-4ccd-bee9-51e159902359"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/drive/My Drive/Datasets/data/Mx_Min_Price.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-4b9f9e856a8f>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/drive/My Drive/Datasets/data/Mx_Min_Price.csv\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    910\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 912\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    575\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 577\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    578\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1659\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1660\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1661\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1662\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1663\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    857\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    860\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/My Drive/Datasets/data/Mx_Min_Price.csv'"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "file_path = \"/content/drive/My Drive/Datasets/data/Mx_Min_Price.csv\"\n",
        "df=pd.read_csv(file_path)\n",
        "df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9LRVsMx1YjLw"
      },
      "outputs": [],
      "source": [
        "file_path = \"/content/drive/My Drive/Datasets/data/Order_details.csv\"\n",
        "df1=pd.read_csv(file_path,encoding='latin1')\n",
        "df1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dyXWabBOspwH"
      },
      "outputs": [],
      "source": [
        "from sklearn.impute import SimpleImputer\n",
        "numeric_cols = df1.select_dtypes(include=['float', 'int']).columns\n",
        "\n",
        "# Apply mean imputation only to numeric columns\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "df1[numeric_cols] = imputer.fit_transform(df1[numeric_cols])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "USEJW1aNGF5m"
      },
      "outputs": [],
      "source": [
        "file_path = \"/content/drive/My Drive/Datasets/data/Property_details.csv\"\n",
        "df2=pd.read_csv(file_path,encoding='latin1')\n",
        "df2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CT66TgaXEA8f"
      },
      "source": [
        "**Data cleaning**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pDRZtNZbeJN_"
      },
      "outputs": [],
      "source": [
        "values_df1 = set(df1['sourceurl'].unique())\n",
        "values_df2 = set(df2['url'].unique())\n",
        "\n",
        "# Intersection\n",
        "overlap = values_df1.intersection(values_df2)\n",
        "print(f\"Number of overlapping values: {len(overlap)}\")\n",
        "print(f\"Percentage of df1 values in overlap: {len(overlap) / len(values_df1) * 100:.2f}%\")\n",
        "print(f\"Percentage of df2 values in overlap: {len(overlap) / len(values_df2) * 100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h1Emlb2yb2my"
      },
      "source": [
        "**FEATURES ENGINEERING**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GUZypUAzca5X"
      },
      "outputs": [],
      "source": [
        "categorical_columns = df1.select_dtypes(include='object').columns.tolist()\n",
        "categorical_columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lxFz7VJKcixD"
      },
      "source": [
        "We will gather usefull information from these columns and generate new columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s18H4Ck3h_e-"
      },
      "outputs": [],
      "source": [
        "#Roomamenities\n",
        "unique_amenities = set()\n",
        "for amenities in df1['roomamenities']:\n",
        "    if isinstance(amenities, str):\n",
        "        for amenity in amenities.split(';'):\n",
        "            amenity = amenity.strip()\n",
        "            if amenity:\n",
        "                unique_amenities.add(amenity)\n",
        "\n",
        "print(unique_amenities)\n",
        "len(unique_amenities)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YrG2DEeemIK3"
      },
      "outputs": [],
      "source": [
        "for amenity in unique_amenities:\n",
        "    df1[amenity] = df1['roomamenities'].apply(lambda x: amenity in x if isinstance(x, str) else False)\n",
        "df1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9913cz4xcDRF"
      },
      "outputs": [],
      "source": [
        "df1.drop(columns=[\"roomamenities\"],inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tiCA1JIhxoOV"
      },
      "outputs": [],
      "source": [
        " df1[\"ratedescription\"].to_list()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aUbj894h8z6h"
      },
      "outputs": [],
      "source": [
        "#Extract Room size from ratedescription\n",
        "import re\n",
        "size_pattern = r'Room\\s*size:\\s*(\\d+)\\s*m[²²]?'\n",
        "\n",
        "# Function to extract room size in square meters\n",
        "def extract_size(description):\n",
        "    if isinstance(description, str):\n",
        "        size_match = re.search(size_pattern, description)\n",
        "        if size_match:\n",
        "            return int(size_match.group(1))\n",
        "    return None\n",
        "\n",
        "# Apply the function to extract room size for each description\n",
        "df1['Roomsize'] = df1['ratedescription'].apply(extract_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dfTdH8hPSYvO"
      },
      "outputs": [],
      "source": [
        "df1[\"Roomsize\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uXuuwhjDvNkR"
      },
      "outputs": [],
      "source": [
        "df1[\"Roomsize\"].isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t1VBczFXh7q7"
      },
      "outputs": [],
      "source": [
        "df1.drop(columns=[\"ratedescription\"],inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y1VMmz_sEubF"
      },
      "outputs": [],
      "source": [
        "df1[\"ratetype\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CVjuW3LPMTnW"
      },
      "outputs": [],
      "source": [
        "#Extract Free Cancellation And Pay at the hotel columns from rate Type\n",
        "df1['Free cancellation'] = df1['ratetype'].str.contains('Free cancellation', case=False, na=False).astype(int)\n",
        "df1['Pay at the hotel'] = df1['ratetype'].str.contains('Pay at the hotel', case=False, na=False).astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5xvO5D1cDHX1"
      },
      "outputs": [],
      "source": [
        "df1[\"Pay at the hotel\"].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B1QsvtHwDyUr"
      },
      "outputs": [],
      "source": [
        "df1['Free cancellation'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hBzjDwyKeBgl"
      },
      "outputs": [],
      "source": [
        "df1.drop(columns=[\"ratetype\"],inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WEDhOi35lTia"
      },
      "outputs": [],
      "source": [
        "def standardize_room_type(room_type):\n",
        "    room_type = room_type.strip()\n",
        "    room_type = re.sub(r'\\s+', ' ', room_type)  # Replace multiple spaces with a single space\n",
        "    return room_type\n",
        "\n",
        "df1['roomtype'] = df1['roomtype'].apply(standardize_room_type)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PKxFxwsOlfXN"
      },
      "outputs": [],
      "source": [
        "df1[\"roomtype\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rxeGzJ48lGMp"
      },
      "outputs": [],
      "source": [
        "# Function to extract features\n",
        "def extract_room_features(room_type):\n",
        "    features = {\n",
        "        'num_beds': 0,\n",
        "        'Single': 0,\n",
        "        'Double': 0,\n",
        "        'Twin': 0,\n",
        "        'Triple': 0,\n",
        "        'Quadruple': 0,\n",
        "        'Suite': 0,\n",
        "        'Deluxe': 0,\n",
        "        'Superior': 0,\n",
        "        'Standard': 0,\n",
        "        'Family': 0,\n",
        "        'Sea_View': 0,\n",
        "        'Balcony': 0,\n",
        "        'Apartment': 0,\n",
        "        'Studio': 0,\n",
        "        'King': 0,\n",
        "        'Queen': 0\n",
        "    }\n",
        "\n",
        "    # Extracting number of beds\n",
        "    bed_match = re.search(r'(\\d+)\\s*(bed|beds)', room_type, re.IGNORECASE)\n",
        "    if bed_match:\n",
        "        features['num_beds'] = int(bed_match.group(1))\n",
        "\n",
        "    # Extract specific terms\n",
        "    if 'Single' in room_type:\n",
        "        features['Single_Room'] = 1\n",
        "    if 'Double' in room_type:\n",
        "        features['Double_Room'] = 1\n",
        "    if 'Twin' in room_type:\n",
        "        features['Twin_Room'] = 1\n",
        "    if 'Triple' in room_type:\n",
        "        features['Triple_Room'] = 1\n",
        "    if 'Quadruple' in room_type:\n",
        "        features['Quadruple_Room'] = 1\n",
        "    if 'Suite' in room_type:\n",
        "        features['Suite_Room'] = 1\n",
        "    if 'Deluxe' in room_type:\n",
        "        features['Deluxe_Room'] = 1\n",
        "    if 'Superior' in room_type:\n",
        "        features['Superior_Room'] = 1\n",
        "    if 'Standard' in room_type:\n",
        "        features['Standard_Room'] = 1\n",
        "    if 'Family' in room_type:\n",
        "        features['Family_Room'] = 1\n",
        "    if 'Sea View' in room_type:\n",
        "        features['Sea_View'] = 1\n",
        "    if 'Balcony' in room_type:\n",
        "        features['Balcony'] = 1\n",
        "    if 'Apartment' in room_type:\n",
        "        features['Apartment'] = 1\n",
        "    if 'Studio' in room_type:\n",
        "        features['Studio_Room'] = 1\n",
        "    if 'King' in room_type:\n",
        "        features['King_Room'] = 1\n",
        "    if 'Queen' in room_type:\n",
        "        features['Queen_Room'] = 1\n",
        "\n",
        "    return pd.Series(features)\n",
        "\n",
        "features_df = df1['roomtype'].apply(extract_room_features)\n",
        "df1 = pd.concat([df1, features_df], axis=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H3Wib85SYBDl"
      },
      "outputs": [],
      "source": [
        "df1.drop(columns=[\"roomtype\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r91v2JcEkHtN"
      },
      "outputs": [],
      "source": [
        "def convert_booleans_to_integers(dfa):\n",
        "    for col in dfa.select_dtypes(include='bool').columns:\n",
        "        dfa[col] = dfa[col].astype(int)\n",
        "    return dfa\n",
        "convert_booleans_to_integers(df1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ewi6aaCFBjQ-"
      },
      "outputs": [],
      "source": [
        "df1.drop(columns=[\"sourceurl\"],inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3_3c_2J-iteN"
      },
      "outputs": [],
      "source": [
        "#since ispromo is object and not bool we independtly convert it to discrete\n",
        "df1[\"ispromo\"]=df1[\"ispromo\"].replace({\"Y\":1,\"N\":0})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bl1d4PVzlnr-"
      },
      "outputs": [],
      "source": [
        "df1[\"ispromo\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pTCdsyu8ARLt"
      },
      "outputs": [],
      "source": [
        "df1[\"closed\"]=df1[\"closed\"].replace({\"Y\":1,\"N\":0})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RDs7DijBBV-k"
      },
      "outputs": [],
      "source": [
        "df1[\"closed\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z0P3baM3nZ_0"
      },
      "outputs": [],
      "source": [
        "df1[\"promoname\"].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_8tDCA86yQWA"
      },
      "outputs": [],
      "source": [
        "#Extract Discount percentage from promoname and one hot encode common phrases into binary columns\n",
        "import re\n",
        "def extract_discount(description):\n",
        "  description=str(description)\n",
        "  match = re.search(r'(\\d+)% discount', description)\n",
        "  return int(match.group(1)) if match else 0\n",
        "\n",
        "# Apply the function to each element in the 'promoname' column\n",
        "df1['discount_percent'] = df1['promoname'].apply(extract_discount)\n",
        "\n",
        "df1[\"discount_percent\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BiRRmjc50T9H"
      },
      "outputs": [],
      "source": [
        "# List of common phrases to encode\n",
        "phrases = ['Early Booking Saver', 'Limited time offer', 'Super Hot Deal', 'Expiring soon', 'Book Now', '72 hr limited sale', 'Last Minute Special', 'Weekend Special', 'Super Saver']\n",
        "df1['promoname'] = df1['promoname'].fillna('')\n",
        "# Create binary columns for each phrase\n",
        "for phrase in phrases:\n",
        "    df1[phrase.replace(' ', '_').replace('.', '')] = df1['promoname'].str.contains(phrase).astype(int)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TgaUG2-NG74G"
      },
      "outputs": [],
      "source": [
        "df1.drop(columns=[\"promoname\"],inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i5MbBJXsnvUc"
      },
      "outputs": [],
      "source": [
        "df1[\"proxyused\"].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o3VgqHT3ptDV"
      },
      "outputs": [],
      "source": [
        "df1.drop(columns=[\"proxyused\"],inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kY8CqWdsn8iN"
      },
      "outputs": [],
      "source": [
        "df1[\"mealinclusiontype\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rIxyzgLV4S6V"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "# Function to standardize text\n",
        "def standardize_text(text):\n",
        "    if isinstance(text, str):\n",
        "        text = text.replace('Free breakfast for {', 'Free breakfast for ')\n",
        "        text = text.replace('}', '')\n",
        "    return text\n",
        "df1['mealinclusiontype'] = df1['mealinclusiontype'].fillna('').apply(standardize_text)\n",
        "\n",
        "def extract_features(text):\n",
        "    features = {\n",
        "        'num_people': 0,  # Default value if text is empty or no number is found\n",
        "        'Free_Breakfast': 0,\n",
        "        'Dinner': 0,\n",
        "        'Lunch': 0,\n",
        "        'Free_WiFi': 0,\n",
        "        'Parking': 0,\n",
        "        'All_Inclusive': 0,\n",
        "        'Free_sauna_access': 0,\n",
        "        'Free_Fitness_Center_Access': 0,\n",
        "        'Beverages': 0\n",
        "    }\n",
        "\n",
        "    # Extract the number of people if the text is not empty\n",
        "    if text:\n",
        "        # Use case-insensitive search for number of people\n",
        "        match = re.search(r'free breakfast for (\\d+)', text, re.IGNORECASE)\n",
        "        if match:\n",
        "            num_people = int(match.group(1))\n",
        "            features['num_people'] = num_people\n",
        "            features['Free_Breakfast'] = num_people\n",
        "        elif re.search(r'free breakfast', text, re.IGNORECASE):\n",
        "            features['Free_Breakfast'] = 1\n",
        "\n",
        "        # Use lowercase for other feature checks\n",
        "        lower_text = text.lower()\n",
        "        features['Dinner'] = 1 if 'dinner' in lower_text else 0\n",
        "        features['Lunch'] = 1 if 'lunch' in lower_text else 0\n",
        "        features['Free_WiFi'] = 1 if 'free wifi' in lower_text else 0\n",
        "        features['Parking'] = 1 if 'parking' in lower_text else 0\n",
        "        features['All_Inclusive'] = 1 if 'all inclusive' in lower_text else 0\n",
        "        features['Free_sauna_access'] = 1 if 'free sauna access' in lower_text else 0\n",
        "        features['Free_Fitness_Center_Access'] = 1 if 'free fitness center access' in lower_text else 0\n",
        "        features['Beverages'] = 1 if 'beverages' in lower_text else 0\n",
        "\n",
        "    return pd.Series(features)\n",
        "\n",
        "# Apply feature extraction\n",
        "features_df = df1['mealinclusiontype'].apply(extract_features)\n",
        "df1 = pd.concat([df1, features_df], axis=1)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SirJeCI09v9Q"
      },
      "outputs": [],
      "source": [
        "df1[\"num_people\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sL4ddEev-KpR"
      },
      "outputs": [],
      "source": [
        "df1[\"Free_Breakfast\"].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5-quEpwQ6ysl"
      },
      "outputs": [],
      "source": [
        "df1.drop(columns=[\"mealinclusiontype\"],inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D0ef2UGUu_b_"
      },
      "source": [
        "Generating Features from dates\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GulYq1nRvw6o"
      },
      "outputs": [],
      "source": [
        "#There are three coulmns containing date.dtcollected,reservation date and input dtcollected"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sTWBBCxmvDfK"
      },
      "outputs": [],
      "source": [
        "df1[\"dtcollected\"]=pd.to_datetime(df1[\"dtcollected\"], infer_datetime_format=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_aSMuK0ixaBx"
      },
      "outputs": [],
      "source": [
        "df1[\"reservation date\"]=pd.to_datetime(df1[\"reservation date\"], infer_datetime_format=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p4z584apxvhD"
      },
      "outputs": [],
      "source": [
        "df1[\"input_dtcollected\"]=pd.to_datetime(df1[\"reservation date\"], infer_datetime_format=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QjjZmLkQvlMj"
      },
      "outputs": [],
      "source": [
        "df1['dtmonth'] = df1['dtcollected'].dt.month\n",
        "df1['dtyear'] = df1['dtcollected'].dt.year\n",
        "df1['dtday'] = df1['dtcollected'].dt.day\n",
        "df1['reservation_month'] = df1['reservation date'].dt.month\n",
        "df1['reservation_year'] = df1['reservation date'].dt.year\n",
        "df1['reservation_day'] = df1['reservation date'].dt.day\n",
        "df1['input_dtmonth'] = df1['input_dtcollected'].dt.month\n",
        "df1['input_dtyear'] = df1['input_dtcollected'].dt.year\n",
        "df1['input_dtday'] = df1['input_dtcollected'].dt.day"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "35vTamlksMb8"
      },
      "outputs": [],
      "source": [
        "df1.drop(columns=[\"dtcollected\",\"reservation date\",\"input_dtcollected\"],inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fcKafearZ1VH"
      },
      "outputs": [],
      "source": [
        "categorical_columns = df1.select_dtypes(include='object').columns.tolist()\n",
        "categorical_columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3LjDvrK31qmh"
      },
      "source": [
        "**FEATURES SELECTION**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJ0ZDqHc_G_6"
      },
      "source": [
        "Identify and drop single-Value columns as they have zero variance and do not contribute to the predictive power of a model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MnKa2A_Q8UoZ"
      },
      "outputs": [],
      "source": [
        "single_value_columns = [col for col in df1.columns if df1[col].nunique() == 1]\n",
        "single_value_columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "29Br83Rk_Wh2"
      },
      "outputs": [],
      "source": [
        "df1.drop(columns=single_value_columns,inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y1OclZ9g37BU"
      },
      "outputs": [],
      "source": [
        "def select_bincolumns(dataframe):\n",
        "  binary_cols=[]\n",
        "  for column in dataframe.columns:\n",
        "    if df1[column].nunique()==2:\n",
        "      binary_cols.append(column)\n",
        "  return binary_cols\n",
        "bin_cols=select_bincolumns(df1)\n",
        "bin_cols"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PalcQUzc4GSK"
      },
      "outputs": [],
      "source": [
        "def select_multicatcol(dataframe,threshold):\n",
        "  multi_cols=[]\n",
        "\n",
        "  for column in dataframe.columns:\n",
        "    unique_values = dataframe[column].nunique()\n",
        "    total_values = len(dataframe[column])\n",
        "    unique_ratio = unique_values / total_values\n",
        "    if unique_ratio<threshold and unique_values>2:\n",
        "      multi_cols.append(column)\n",
        "\n",
        "  return multi_cols\n",
        "multi_columns=select_multicatcol(df1,0.05)\n",
        "multi_columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H-MeOICN3HQx"
      },
      "outputs": [],
      "source": [
        "numerical_columns = df1.select_dtypes(include=['number']).columns.tolist()\n",
        "numerical_columns\n",
        "\n",
        "numerical_columns_excluding_binary_multiclass = [\n",
        "    col for col in numerical_columns if col not in bin_cols and col not in multi_columns\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SxuvmZPY4dfB"
      },
      "outputs": [],
      "source": [
        "numerical_columns_excluding_binary_multiclass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ypn3OcG_1tbe"
      },
      "outputs": [],
      "source": [
        "import scipy.stats as stats\n",
        "def pearson_corr(a,b):\n",
        "   corr, p_value = stats.pearsonr(a, b)\n",
        "   return corr, p_value\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p7wguDHk1-hO"
      },
      "outputs": [],
      "source": [
        "# Interpret the results\n",
        "dropped_num_cols=[]\n",
        "alpha = 0.05\n",
        "for column in numerical_columns_excluding_binary_multiclass:\n",
        "  corr,p_value=pearson_corr(df1[column],df1[\"onsiteprice\"])\n",
        "\n",
        "  if p_value < alpha:\n",
        "      print(\"There is a significant correlation between variable1 and variable2.\")\n",
        "  else:\n",
        "      print(\"There is no significant correlation between variable1 and variable2.\")\n",
        "      dropped_num_cols.append(column)\n",
        "dropped_num_cols"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dd16XuLRqkm_"
      },
      "outputs": [],
      "source": [
        "#Drop id column\n",
        "df1.drop(columns=[\"id\"],inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1xULCCXguk2D"
      },
      "source": [
        "If features are binary variable and target is continous then t-test is applied to determine if there is a relationship between the two variables\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "inRzSOsByjrT"
      },
      "outputs": [],
      "source": [
        "def select_bincolumns(dataframe):\n",
        "  binary_cols=[]\n",
        "  for column in dataframe.columns:\n",
        "    if df1[column].nunique()==2:\n",
        "      binary_cols.append(column)\n",
        "  return binary_cols\n",
        "bin_cols=select_bincolumns(df1)\n",
        "bin_cols"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cxs-s4RLHvuA"
      },
      "outputs": [],
      "source": [
        "from scipy.stats import ttest_ind\n",
        "def perform_t_test(group1, group2, alpha=0.05):\n",
        "    t_stat, p_value = ttest_ind(group1, group2)\n",
        "    return t_stat, p_value\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h9J2pwTtIfiG"
      },
      "outputs": [],
      "source": [
        "alpha=0.05\n",
        "t_test_results={}\n",
        "dropped_columns=[]\n",
        "for column in bin_cols:\n",
        "  group1=df1[df1[column]==0][\"onsiteprice\"]\n",
        "  group2=df1[df1[column]==1][\"onsiteprice\"]\n",
        "  t_stat, p_value=perform_t_test(group1, group2)\n",
        "  t_test_results[column] = {'t_statistic': t_stat, 'p_value': p_value}\n",
        "  if p_value >= alpha:\n",
        "    result = \"Reject H0: There is a significant difference between means.Drop Feature\"\n",
        "    dropped_columns.append(column)\n",
        "  else:\n",
        "    result = \"Fail to reject H0: No significant difference between means.Select Feature\"\n",
        "\n",
        "# Display results\n",
        "for col, result in t_test_results.items():\n",
        "    print(f\"Column: {col}, T-statistic: {result['t_statistic']}, P-value: {result['p_value']}\")\n",
        "    print()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iC04C1n6M-iJ"
      },
      "outputs": [],
      "source": [
        "dropped_columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nPpgmQpmrN2j"
      },
      "outputs": [],
      "source": [
        "#dropping these columns\n",
        "df1.drop(columns=dropped_columns,inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ykpw0Hs65tjW"
      },
      "source": [
        "**If the target variable is continous and features are mulit-class categorical we use ANOVA Test to find wether or not there is a relationship**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pPyvV6nIVcQt"
      },
      "outputs": [],
      "source": [
        "def select_multicatcol(dataframe,threshold):\n",
        "  multi_cols=[]\n",
        "\n",
        "  for column in dataframe.columns:\n",
        "    unique_values = dataframe[column].nunique()\n",
        "    total_values = len(dataframe[column])\n",
        "    unique_ratio = unique_values / total_values\n",
        "    if unique_ratio<threshold and unique_values>2:\n",
        "      multi_cols.append(column)\n",
        "\n",
        "  return multi_cols\n",
        "multi_columns=select_multicatcol(df1,0.05)\n",
        "multi_columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v5mTgA5sF-7T"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from scipy.stats import f_oneway\n",
        "\n",
        "# Function to perform ANOVA\n",
        "def perform_anova(dataframe, categorical_column, continuous_column, alpha=0.05):\n",
        "    dataframe[categorical_column] = dataframe[categorical_column].astype('category')\n",
        "    groups = [dataframe[dataframe[categorical_column] == category][continuous_column] for category in dataframe[categorical_column].cat.categories]\n",
        "    f_statistic, p_value = f_oneway(*groups)\n",
        "    # Decision rule\n",
        "    if p_value < alpha:\n",
        "        decision = \"Keep Feature\"\n",
        "    else:\n",
        "        decision = \"Drop Feature\"\n",
        "\n",
        "    return {\n",
        "        'F-statistic': f_statistic,\n",
        "        'p-value': p_value,\n",
        "        'decision': decision\n",
        "    }\n",
        "\n",
        "# Example DataFrame and variables\n",
        "alpha = 0.05\n",
        "anova_results = {}\n",
        "continuous_column = \"onsiteprice\"  # Replace with your actual continuous target column name\n",
        "columns_to_drop = []\n",
        "\n",
        "# Loop through each multi-class categorical column\n",
        "for column in multi_columns:  # Replace with your actual list of multi-class columns\n",
        "    results = perform_anova(df1, column, continuous_column)\n",
        "    anova_results[column] = results\n",
        "\n",
        "    print(f\"Column: {column}, F-statistic: {results['F-statistic']}, P-value: {results['p-value']}\")\n",
        "    print(f\"Decision: {results['decision']}\")\n",
        "    print()\n",
        "    # Append column to drop list if decision is to drop feature\n",
        "    if results['decision'] == \"Drop Feature\":\n",
        "        columns_to_drop.append(column)\n",
        "\n",
        "# Display all ANOVA results\n",
        "print(\"Columns to drop:\", columns_to_drop)\n",
        "for col, result in anova_results.items():\n",
        "    print(f\"Column: {col}, F-statistic: {result['F-statistic']}, P-value: {result['p-value']}\")\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HDBRSBneXwU8"
      },
      "outputs": [],
      "source": [
        "columns_to_drop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jqct6NL_r5l7"
      },
      "outputs": [],
      "source": [
        "df1.drop(columns=columns_to_drop,inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KnNCAcXumVmd"
      },
      "outputs": [],
      "source": [
        "# dfa=df1.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7c2ecpOxlQzb"
      },
      "outputs": [],
      "source": [
        "# from sklearn.impute import KNNImputer\n",
        "# new_df = dfa[[\"Roomsize\"]]\n",
        "# knn_imputer = KNNImputer(n_neighbors=5)  # n_neighbors is the number of neighboring samples to use\n",
        "# imputed_array = knn_imputer.fit_transform(new_df)\n",
        "\n",
        "# # Create a new DataFrame with the imputed values\n",
        "# imputed_df = pd.DataFrame(imputed_array, columns=[\"Roomsize\"])\n",
        "\n",
        "# # Replace the original 'roomsize' column with the imputed values\n",
        "# dfa['Roomsize'] = imputed_df['Roomsize']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fiv2dzki6Em6"
      },
      "outputs": [],
      "source": [
        "df1.columns.to_list()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mgoDYLhD5Znw"
      },
      "outputs": [],
      "source": [
        "# from sklearn.impute import KNNImputer\n",
        "# new_df = df1[[\"Roomsize\"]]\n",
        "# knn_imputer = KNNImputer(n_neighbors=5)  # n_neighbors is the number of neighboring samples to use\n",
        "# imputed_array = knn_imputer.fit_transform(new_df)\n",
        "\n",
        "# # Create a new DataFrame with the imputed values\n",
        "# imputed_df = pd.DataFrame(imputed_array, columns=[\"Roomsize\"])\n",
        "\n",
        "# # Replace the original 'roomsize' column with the imputed values\n",
        "# df1['Roomsize'] = imputed_df['Roomsize']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GLt4GIHofOn7"
      },
      "outputs": [],
      "source": [
        "df1[\"roomtype\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CIFh2LPWn394"
      },
      "outputs": [],
      "source": [
        "df1.drop(columns=[\"roomtype\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yQaIegO794cy"
      },
      "outputs": [],
      "source": [
        "X=df1.drop(columns=[\"onsiteprice\"])\n",
        "y=df1[\"onsiteprice\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r2KaMUCS9t75"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9sr0E7ax-c_Z"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "lr=LinearRegression()\n",
        "lr.fit(X_train,y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ucJzi_SI6uN1"
      },
      "outputs": [],
      "source": [
        "# import pandas as pd\n",
        "\n",
        "# # Assuming your final DataFrame is df_final\n",
        "# df1.to_csv('/content/drive/My Drive/Datasets/data/final_dataframe.csv', index=False)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}